{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Twitter the-algorithm source code with LangChain, GPT4 and Activeloop's Deep Lake\n",
    "In this tutorial, we are going to use Langchain + Activeloop's Deep Lake with GPT4 to analyze the code base of the twitter algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define OpenAI embeddings, Deep Lake multi-modal vector store api and authenticate. For full documentation of Deep Lake please follow [docs](https://docs.activeloop.ai/) and [API reference](https://docs.deeplake.ai/en/latest/).\n",
    "\n",
    "Authenticate into Deep Lake if you want to create your own dataset and publish it. You can get an API key from the [platform](https://app.activeloop.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = OpenAIEmbeddings(disallowed_special=())\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text:v1.5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disallowed_special=() is required to avoid `Exception: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte` from tiktoken for some repositories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Index the code base (optional)\n",
    "You can directly skip this part and directly jump into using already indexed dataset. To begin with, first we will clone the repository, then parse and chunk the code base and use OpenAI indexing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all files inside the repository"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, chunk the files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the indexing. This will take about ~4 mins to compute embeddings and upload to Activeloop. You can then publish the dataset to be public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optional`: You can also use Deep Lake's Managed Tensor Database as a hosting service and run queries there. In order to do so, it is necessary to specify the runtime parameter as {'tensor_db': True} during the creation of the vector store. This configuration enables the execution of queries on the Managed Tensor Database, rather than on the client side. It should be noted that this functionality is not applicable to datasets stored locally or in-memory. In the event that a vector store has already been created outside of the Managed Tensor Database, it is possible to transfer it to the Managed Tensor Database by following the prescribed steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username = \"davitbun\"  # replace with your username from app.activeloop.ai\n",
    "# db = DeepLake(\n",
    "#     dataset_path=f\"hub://{username}/twitter-algorithm\",\n",
    "#     embedding_function=embeddings,\n",
    "#     runtime={\"tensor_db\": True}\n",
    "# )\n",
    "# db.add_documents(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Question Answering on Twitter algorithm codebase\n",
    "First load the dataset, construct the retriever, then construct the Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20873\n"
     ]
    }
   ],
   "source": [
    "# aproximadamente 37 minutos\n",
    "faiss_path = \"faiss_index\"\n",
    "if os.path.exists(faiss_path):\n",
    "    db = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify user defined functions using [Deep Lake filters](https://docs.deeplake.ai/en/latest/deeplake.core.dataset.html#deeplake.core.dataset.Dataset.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if \"com.google\" in x[\"text\"].data()[\"value\"]:\n",
    "        return False\n",
    "\n",
    "    # filter based on path e.g. extension\n",
    "    metadata = x[\"metadata\"].data()[\"value\"]\n",
    "    return \"scala\" in metadata[\"source\"] or \"py\" in metadata[\"source\"]\n",
    "\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "model = Ollama(model=\"mistral:7b\")\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Questão**: O que é o método similarity_search_with_score? \n",
      "\n",
      "**Resposta**:  The method `similarity_search_with_score` is a function provided by the Dingo library, which returns a list of documents along with their scores based on their similarity to a given query. This function uses vector search algorithms and natural language processing techniques to find the most relevant documents in a collection. The scores represent how closely the documents match the query in terms of context relevance, factual accuracy, response completeness, sub-query completeness, context reranking, and context conciseness. By default, it returns up to 4 documents with their respective scores. You can also pass optional arguments like `search_params` to filter on metadata or `timeout` to specify a maximum search duration. If you only need the documents without their scores, you can call the simpler function `similarity_search`. \n",
      "\n",
      "-> **Questão**: O que é a opção maximal_marginal_relevance? \n",
      "\n",
      "**Resposta**:  The `maximal_marginal_relevance` option is not a valid argument for the `similarity_search_with_score` function in Qdrant. It appears that you might be confusing this with another relevance strategy used by Qdrant, which is called \"Maximal Marginal Relevance Feedback (MMR)\".\n",
      "\n",
      "MMR is a ranking technique that optimizes for both query relevance and document diversity among the search results. This means that MMR tries to return documents that are most similar to the query while also ensuring that the selected documents cover a wide range of topics, rather than repeating the same information multiple times.\n",
      "\n",
      "However, the `similarity_search_with_score` function is used to retrieve documents based on their similarity scores to a given query and does not directly support the MMR strategy. If you want to use MMR in Qdrant, you can consider using the `maximal_marginal_relevance_search` function instead.\n",
      "\n",
      "Here's an example of how to use `maximal_marginal_relevance_search`:\n",
      "\n",
      "```python\n",
      "import json\n",
      "from qdrant_client import QdrantClient, Document, MetadataFilter\n",
      "from typing import List\n",
      "\n",
      "# Initialize the Qdrant client\n",
      "qdrant = QdrantClient()\n",
      "\n",
      "# Define your documents and index them\n",
      "documents = [\n",
      "    {\"id\": 1, \"content\": \"The quick brown fox jumps over the lazy dog\"},\n",
      "    {\"id\": 2, \"content\": \"A red fox jumps over a rabbit hole\"},\n",
      "    {\"id\": 3, \"content\": \"A snow leopard is a large cat native to Central Asia\"}\n",
      "]\n",
      "for doc in documents:\n",
      "    qdrant.upsert([Document(data=doc)], index=\"my_index\")\n",
      "\n",
      "# Perform a maximal marginal relevance search\n",
      "search_results = qdrant.maximal_marginal_relevance_search(\"fox\", k=3, lambda_mult=0.5)\n",
      "print(json.dumps(search_results, indent=2))\n",
      "```\n",
      "\n",
      "In this example, we perform a maximal marginal relevance search for the term \"fox\" and return three documents that are most similar to the query while also ensuring some level of diversity in the search results (specified by the `lambda_mult` parameter). \n",
      "\n",
      "-> **Questão**: Para que serve a classe ConversationalRetrievalChain? \n",
      "\n",
      "**Resposta**:  The `ConversationalRetrievalChain` class is a part of Langchain library and is designed to retrieve information from a given dataset based on a conversational interaction between the user and an assistant. It's responsible for processing user input, generating a question to be answered by the language model, retrieving relevant documents using the `BaseRetriever` instance, and combining those documents into a single context for the language model.\n",
      "\n",
      "This class is particularly useful when dealing with large datasets or situations where the desired information might not be present in a single document but can be gleaned from multiple sources. Additionally, it allows for a conversational interaction between the user and the assistant, making the retrieval process more engaging and user-friendly. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"O que é o método similarity_search_with_score?\",\n",
    "    \"O que é a opção maximal_marginal_relevance?\",\n",
    "    \"Para que serve a classe ConversationalRetrievalChain?\"\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Questão**: {question} \\n\")\n",
    "    print(f\"**Resposta**: {result['answer']} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> **Questão**: O que é o método similarity_search_with_score? \n",
    "\n",
    "**Resposta**:  The method `similarity_search_with_score` is a function provided by the Dingo library, which returns a list of documents along with their scores based on their similarity to a given query. This function uses vector search algorithms and natural language processing techniques to find the most relevant documents in a collection. The scores represent how closely the documents match the query in terms of context relevance, factual accuracy, response completeness, sub-query completeness, context reranking, and context conciseness. By default, it returns up to 4 documents with their respective scores. You can also pass optional arguments like `search_params` to filter on metadata or `timeout` to specify a maximum search duration. If you only need the documents without their scores, you can call the simpler function `similarity_search`. \n",
    "\n",
    "-> **Questão**: O que é a opção maximal_marginal_relevance? \n",
    "\n",
    "**Resposta**:  The `maximal_marginal_relevance` option is not a valid argument for the `similarity_search_with_score` function in Qdrant. It appears that you might be confusing this with another relevance strategy used by Qdrant, which is called \"Maximal Marginal Relevance Feedback (MMR)\".\n",
    "\n",
    "MMR is a ranking technique that optimizes for both query relevance and document diversity among the search results. This means that MMR tries to return documents that are most similar to the query while also ensuring that the selected documents cover a wide range of topics, rather than repeating the same information multiple times.\n",
    "\n",
    "However, the `similarity_search_with_score` function is used to retrieve documents based on their similarity scores to a given query and does not directly support the MMR strategy. If you want to use MMR in Qdrant, you can consider using the `maximal_marginal_relevance_search` function instead.\n",
    "\n",
    "Here's an example of how to use `maximal_marginal_relevance_search`:\n",
    "\n",
    "```python\n",
    "import json\n",
    "from qdrant_client import QdrantClient, Document, MetadataFilter\n",
    "from typing import List\n",
    "\n",
    "# Initialize the Qdrant client\n",
    "qdrant = QdrantClient()\n",
    "\n",
    "# Define your documents and index them\n",
    "documents = [\n",
    "    {\"id\": 1, \"content\": \"The quick brown fox jumps over the lazy dog\"},\n",
    "    {\"id\": 2, \"content\": \"A red fox jumps over a rabbit hole\"},\n",
    "    {\"id\": 3, \"content\": \"A snow leopard is a large cat native to Central Asia\"}\n",
    "]\n",
    "for doc in documents:\n",
    "    qdrant.upsert([Document(data=doc)], index=\"my_index\")\n",
    "\n",
    "# Perform a maximal marginal relevance search\n",
    "search_results = qdrant.maximal_marginal_relevance_search(\"fox\", k=3, lambda_mult=0.5)\n",
    "print(json.dumps(search_results, indent=2))\n",
    "```\n",
    "\n",
    "In this example, we perform a maximal marginal relevance search for the term \"fox\" and return three documents that are most similar to the query while also ensuring some level of diversity in the search results (specified by the `lambda_mult` parameter). \n",
    "\n",
    "-> **Questão**: Para que serve a classe ConversationalRetrievalChain? \n",
    "\n",
    "**Resposta**:  The `ConversationalRetrievalChain` class is a part of Langchain library and is designed to retrieve information from a given dataset based on a conversational interaction between the user and an assistant. It's responsible for processing user input, generating a question to be answered by the language model, retrieving relevant documents using the `BaseRetriever` instance, and combining those documents into a single context for the language model.\n",
    "\n",
    "This class is particularly useful when dealing with large datasets or situations where the desired information might not be present in a single document but can be gleaned from multiple sources. Additionally, it allows for a conversational interaction between the user and the assistant, making the retrieval process more engaging and user-friendly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
